---
title: "A Universal LLM-based Framework for Material Discovery: Knowledge Distillation from 30B to 4B Models"
collection: publications
category: Current Research
permalink: /publication/2026-universal-llm-distillation
date: 2026-01-12
venue: 'Working Paper / In Preparation'
excerpt: >
  We present a highly transferable and reusable LLM-based pipeline designed to accelerate scientific discovery across diverse domains. 
  Demonstrated through catalyst screening for Lithium-Sulfur batteries, the workflow follows a hierarchical logic: 
  (1) Domain-Specific Adaptation: Fine-tuning a 30B-parameter "Teacher" model on specialized literature to capture deep chemical insights. 
  (2) Knowledge Distillation: Transferring expert reasoning to a lightweight 4B "Student" model for multi-dimensional scoring (activity, stability, etc.). 
  (3) High-Throughput Screening: Utilizing the 4B model for rapid, large-scale candidate evaluation. 
  Crucially, this modular framework is domain-agnostic; by replacing the fine-tuning dataset, it can be seamlessly migrated to other materials science or chemistry fields, providing a universal solution for AI-driven scientific research (AI4S).
---